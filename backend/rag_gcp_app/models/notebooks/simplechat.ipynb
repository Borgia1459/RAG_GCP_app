{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "#understanding tokens in LLM\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = 'The ball is on the table.'\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer.encode(text, return_tensors='pt')\n",
    "print(encoded_input) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b93ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(encoded_input[0])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import DocugamiLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "gcp_key = os.getenv('GOOGLE_GEMINI_API_KEY')\n",
    "\n",
    "# Check if the API key is loaded\n",
    "if not gcp_key:\n",
    "    raise ValueError(\"GOOGLE_GEMINI_API_KEY not found in environment variables.\")\n",
    "print(\"GOOGLE_GEMINI_API_KEY loaded successfully: \", gcp_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf700000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.oauth2 import service_account\n",
    "import os \n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = '../../acoustic-realm-385717-f4e225be9213.json'\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '../../acoustic-realm-385717-f4e225be9213.json'\n",
    ")\n",
    "\n",
    "project = 'acoustic-realm-385717'\n",
    "location = 'us-central1'\n",
    "aiplatform.init(project=project, location=location, credentials=credentials)\n",
    "\n",
    "models = aiplatform.Model.list()\n",
    "for model in models: \n",
    "    print(f\"Model ID: {model.name}, Model Display Name: {model.display_name}, Model Version: {model.version_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4cf2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.embeddings.google_palm import GooglePalmEmbeddings\n",
    "from langchain.llms import GooglePalm\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "import os \n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "def load_file():\n",
    "    loader = DirectoryLoader(path=files_loc, glob=\"**/*.pdf\")\n",
    "    loaded_file = loader.load()\n",
    "    print(f\"Loaded {len(loaded_file)} files.\")\n",
    "    return loaded_file\n",
    "\n",
    "def splitting(loaded_file):\n",
    "    text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=2000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(loaded_file)\n",
    "    #print(f'sample text: \\n {chunks[34].page_content[:500]}')\n",
    "    return chunks \n",
    "#def generate_embeddings()\n",
    "def text_embedding(split_texts,embeddings):\n",
    "    \n",
    "    text_contents = [text.page_content for text in split_texts]\n",
    "    document_embeddings = embeddings.embed_documents(text_contents)\n",
    "    \n",
    "    try: \n",
    "        print(f'One of the embeddings: {document_embeddings[34]}')\n",
    "        print('Embedding generation successful.')\n",
    "    except Exception as e:\n",
    "        print(f\"Error during embedding: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return document_embeddings  \n",
    "\n",
    "def vectorDB(split_texts, embeddings):\n",
    "    #create a vectorDB using langchain.chroma\n",
    "    '''try:\n",
    "        vector_store = Chroma.from_documents(\n",
    "                                             collection_name= 'Seneca_Collection',\n",
    "                                             documents=split_texts, \n",
    "                                             embedding=embeddings, \n",
    "                                             persist_directory=\"../data\")\n",
    "        print(\"Vector store created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {e}\")\n",
    "        return None'''\n",
    "    vector_store = Chroma.as_retriever() \n",
    "    return vector_store\n",
    "\n",
    "def similaritySearch(vctordb):\n",
    "    # form similarity search which will be used to retrieve relevant documents\n",
    "    similarity_search = vector_database.similarity_search(\"What is the main theme of the document?\", k=3)\n",
    "    print(f\"Similarity search results: {similarity_search}\")\n",
    "\n",
    "def retrievel_qa_chain(vector_store):\n",
    "    llm = GooglePalm(model_name=\"gemini-1.5-pro\", google_api_key=gcp_key) \n",
    "    print(\"Language model initialized.\") \n",
    "    print(\"Creating retrieval QA chain...\") \n",
    "    retrievel_qa = create_retrieval_chain(llm=llm, retriever=vector_store.as_retriever())\n",
    "    print(\"Retrieval QA chain created.\") \n",
    "    return retrievel_qa\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    gcp_key = os.getenv('GOOGLE_GEMINI_API_KEY') \n",
    "    files_loc = '../data/'\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=gcp_key)\n",
    "    loaded_files = load_file()\n",
    "    split_texts = splitting(loaded_files)\n",
    "    embeddings_texts = text_embedding(split_texts,embeddings)\n",
    "    vector_database = vectorDB(split_texts, embeddings)\n",
    "    similarity_search = similaritySearch(vector_database)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f3d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.chat_models import ChatGooglePalm\n",
    " \n",
    "# Function to chat with the model\n",
    "def chat_with_model(query):\n",
    "    chat = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", google_api_key=gcp_key)\n",
    "    messages = [SystemMessage(content='You are an AI assistant. Provide concise and accurate answers to user queries.'),\n",
    "                HumanMessage(content=query)] \n",
    "    \n",
    "    response = chat([HumanMessage(content=query)])\n",
    "    answer = chat.invoke([HumanMessage(content=query)])\n",
    "    return print(answer) \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    query_tg = 'how many oceans are there on earth?'\n",
    "    response = chat_with_model(query_tg) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GCP_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
